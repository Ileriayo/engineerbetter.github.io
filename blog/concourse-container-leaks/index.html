<!doctype html><html lang=pl><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><title>Concourse Container Leak | EngineerBetter | More than Cloud Platform specialists</title><link rel=stylesheet href="/css/main.css?v=4"><link rel=stylesheet href=/css/syntax.css><link rel=icon type=image/png href=/img/favicon.png><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-74442484-1","auto"),ga("send","pageview")</script><script src=//load.sumome.com/ data-sumo-site-id=7c63408cd1c29faa63d20123ff629d524df6674695e4305a0e602f2bbf431038 async></script>
<script type=text/javascript>var _iub=_iub||[];_iub.csConfiguration={banner:{textColor:"#dadada",backgroundColor:"#5A5A5A"},lang:"en",siteId:939573,cookiePolicyId:8251825}</script><script type=text/javascript src=//cdn.iubenda.com/cookie_solution/safemode/iubenda_cs.js async></script></head><body><section id=menu-nav-top class="menu fixed-menu clearfix"><div class=menu_contact><p class=menu_contact-number>+44 (0) 20 7846 0140</p><a href=mailto:contact@engineerbetter.com class=menu_contact-us>Contact us</a></div><div class="menu_header clearfix"><button class=menu_header-button>
<span class="icon-bar top-bar"></span>
<span class="icon-bar middle-bar"></span>
<span class="icon-bar bottom-bar"></span></button>
<a href=/ class=menu_header-logo><img class=logo-header-img alt=Brand src=/img/engineer-better-logo.svg></a></div></section><div class=menu_list><button class=menu_list-close><p>&#x2716;</p></button><div class=menu_list-inner-wrap><p class=menu_list-phone>+44 (0) 20 7846 0140</p><a class=menu_list-contact-btn href=mailto:contact@engineerbetter.com>Contact us&nbsp;<span>&#xe900;</span></a><ul class=menu_list-links><li><a href=/how-we-work/>How we work</a></li><li><a href=/our-services/>Our services</a></li><li><a href=/success-stories/>Success stories</a></li><li><a href=/why-cloud-native/>why cloud-native?</a></li><li><a href=/about-us/>About us</a></li><li><a href=/blog/>Blog</a></li></ul><div class=menu_list-icons><a href=https://twitter.com/EngineerBetter><img src=/img/twitter.svg alt="Twitter profile"></a>
<a href=https://github.com/EngineerBetter><img src=/img/github.svg alt="Github profile"></a></div><div class=menu_list-bot-menu><a href=/blog/>Blog</a><span>|</span><a href=/join-our-team/>Careers</a><span>|</span><a href=//www.iubenda.com/privacy-policy/8251825 class="iubenda-nostyle no-brand iubenda-embed" title="Privacy Policy">Privacy</a><script type=text/javascript>(function(e,t){var n=function(){var e=t.createElement("script"),n=t.getElementsByTagName("script")[0];e.src="//cdn.iubenda.com/iubenda.js",n.parentNode.insertBefore(e,n)};e.addEventListener?e.addEventListener("load",n,!1):e.attachEvent?e.attachEvent("onload",n):e.onload=n})(window,document)</script></div></div></div><section class="menu-banner-top_wrap parallax is-page"><div class=menu-banner-top><div class=menu-banner-top_contact><a class=menu-banner-top_number href="tel:+44 (0) 20 7846 0140">+44 (0) 20 7846 0140</a>
<a class=menu-banner-top_contact-us href=mailto:contact@engineerbetter.com>Contact us</a></div><div class=menu-banner-top_btn-wrap><button class=menu_header-button>
<span class="icon-bar top-bar"></span>
<span class="icon-bar middle-bar"></span>
<span class="icon-bar bottom-bar"></span></button></div><div class="menu-banner-top_logo clearfix"><a href=/ class=home-banner_logo-img><img class=logo-img alt=Brand src=/img/engineer-better-logo.svg></a></div></div><div class=home-banner><div class="home-banner_list clearfix"><div class=home-banner_list-category><ul><li><a href=ileri.com/how-we-work/>How we work</a></li><li><a href=ileri.com/our-services/>Our services</a></li><li><a href=ileri.com/success-stories/>Success Stories</a></li><li><a href=ileri.com/why-cloud-native/>Why Cloud-Native?</a></li><li><a href=ileri.com/about-us/>About us</a></li><li><a href=ileri.com/blog/>Blog</a></li></ul></div><div class=home-banner_textarea><h1 class=home-banner_textarea-header>Our <strong>blog&nbsp;<span>&#xe900;</span></strong></h1><div class=home-banner_textarea-wrap><p class=home-banner_textarea-description>Get the very latest updates about recent projects, team updates, thoughts and industry news from our team of EngineerBetter experts.</p></div></div></div></div></section><section class=blog-page><div class=blog-page_container><h1>Concourse Container Leak</h1><p class=blog-page_details>Oct 27, 2016<span>|</span> Daniel Jones</p><div class=blog-page_image><img src=/img/blog/containers.jpg></div><p>A customer of ours recently had trouble with their Concourse instance which resulted in us raising a <a href=https://github.com/concourse/baggageclaim/issues/6>GitHub issue</a> describing how containers may get orphaned and that users may experience the error message <code>insufficient subnets remaining in the pool</code>.</p><p>This can mostly easily be remedied by performing <code>bosh recreate</code> on the Worker VM, or if you&rsquo;re not sensible enough to have deployed your Concourse with BOSH, by locating and killing all the orphaned container processes. The debugging journey was rather fun and exposed some of the innards of Concourse and Garden, so we&rsquo;ve decided to share it.</p><p>Before going any further I should point out that all of the Bash-fu is the work of my colleague <a href=https://twitter.com/jpluscplusm>Jonathan Matthews</a>.</p><p>Our first instinct was to check <code>fly containers</code> to see how many were in existence. We only saw 14 in total, which shouldn&rsquo;t be enough to starve Concourse of any resources. So we then used <code>fly workers</code> to get a second view of how many containers were active:</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ fly -t target workers
name                                  containers  platform  tags  team
0a04d3fd-d485-4976-b4a0-84e12c37c05c  242         linux     none  none
</code></pre><p>A quick bit of Googling (other search engines are available) led us to <a href=https://github.com/cloudfoundry/guardian/issues/53>a GitHub issue citing the same error message</a>, but with no conclusive resolution. The issue pointed out that there&rsquo;s a hard limit in the defaults of Garden (the underlying container-running implementation behind Concourse) of 250 containers, and that there&rsquo;s also a limit on the subnets that can be assigned to those containers. This at least explained why the error message was about subnets.</p><p>Our next question was: &ldquo;<em>What are all these containers doing?</em>&rdquo; This Concourse instance has around 10 modest pipelines, each with no more that 5 resources to be polled. A quick survey of the teams yielded that no-one had set <code>check_every</code> on their resources to anything other than the default, so we were able to rule out a user DoSing Garden with excessive resource polling.</p><p>We <code>bosh ssh</code>&rsquo;d onto the Worker VM to investigate further, first poking around in the Garden logs in <code>/var/vcap/sys/log/garden</code>. We then downloaded the very-handy <a href=https://github.com/contraband/gaol><code>gaol</code> CLI</a> to interact with the Garden server running on the Worker - it connected automatically, presumably defaulting to conventional settings for the server.</p><p>We used <code>gaol list</code> to have a look at the containers this Garden instance was running, and found a very long list of UUIDs. All of the UUIDs from <code>fly containers</code> appeared in <code>gaol list</code>, which would have to be a Universe-breaking coincidence if Concourse wasn&rsquo;t exposing Garden UUIDs via its UI. We&rsquo;re rather glad that it does, as this made debugging a little easier.</p><p><code>gaol properties</code> revealed that our containers had a <code>grace-time</code> of <code>300000000000</code>. The only snafu is that there are no <em>units</em> specified, so we had to trawl through the Garden and Concourse code to try and find out what the default unit of measurement was. Sadly it was quicker to work out that 5 minutes is equal to 300,000,000,000 nanoseconds than it was to find any conclusive proof, so we made the assumption that Concourse was setting the grace time of Garden containers in line with its advertised TTL.</p><p>At this point we know that Concourse says its Worker has many containers, possibly more than we think it should. Garden concurs with this point of view. Because of the limitations of <code>fly</code> we can&rsquo;t see all containers across all teams.</p><p>We needed to try and determine whether the containers alive in Garden were &lsquo;valid&rsquo; or not: should they be there? Are they for active running tasks?</p><p><code>gaol shell</code> allowed us to get a shell session in any given container. The first few we picked at random seemed quite innocuous, mostly Concourse resources being checked. New containers were being spun up as others were being torn down, which made investigating them all a game of whack-a-mole. We needed to instead do coarse-grained diff over the containers - how many would still be alive after the stated TTL of 5 minutes? We knew that few jobs in any of the teams&rsquo; pipelines lasted longer than a few minutes, so we were unlikely to see hours-long tasks.</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ gaol list &gt; containers-before
# 15 minutes later...
$ gaol list &gt; containers-after
$ diff --side &lt;(sort containers-before) &lt;(sort containers-after) | grep -c -e &#39;&lt;&#39; -e &#39;&gt;&#39; -e &#39;|&#39;
</code></pre><p>This yielded the fact that the majority of containers were living longer that 15 minutes. It only then dawned on us that we could simply use <code>ps</code> to see how long some of these processes had been kicking around for:</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ ps aux | grep run[c]
</code></pre><p>Some of these containers had been kicking around for a week! What&rsquo;s more is that the <code>ps</code> output included the UUID of each of them, so we could use <code>goal shell</code> to get inside and see what was happening. Or so we thought.</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ gaol shell a65b0643-4e41-4081-634c-0256fe4e91e8
error: hijack: Backend error: Exit status: 500, message: {&#34;Type&#34;:&#34;&#34;,&#34;Message&#34;:&#34;unable to find user root: no matching entries in passwd file&#34;,&#34;Handle&#34;:&#34;&#34;}
</code></pre><p>The long-living containers seemed to be in some state of distress. Were their volumes missing? <code>gaol properties</code> tells us the UUIDs of volumes mounted into each container, and we happen to know that those volumes are managed by Baggage Claim and live in <code>/var/vcap/data/baggageclaim/volumes/</code>.</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ gaol properties eb8f2bbf-86e1-4603-4881-548c9133aa62
# lots of stuff...
concourse:volumes [&#34;88914c83-fc71-4358-4f3f-b886429f9013&#34;,&#34;1a657656-9647-4bee-54be-8505eb323b9e&#34;]
# lots of stuff...
</code></pre><p>We now had a place to go hunting for further information:</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ ll /var/vcap/data/baggageclaim/volumes/live/88914c83-fc71-4358-4f3f-b886429f9013
total 8
drwxr-xr-x 1 root root    58 Oct 19 19:07 ./
drwxr-xr-x 1 root root 17424 Oct 27 15:58 ../
-rw-r--r-- 1 root root   461 Oct 19 19:08 properties.json
-rw-r--r-- 1 root root    36 Oct 21 14:29 ttl.json
drwxr-xr-x 1 root root   130 Oct 19 19:08 volume/

$ cat properties.json
{
   &#34;initialized&#34;:&#34;yep&#34;,
   &#34;resource-params&#34;:&#34;04f8ff2682604862e405bf88de102ed7710ac45c1205957625e4ee3e5f5a2241e453614acc451345b91bafc88f38804019c7492444595674e94e8cf4be53817f&#34;,
   &#34;resource-source&#34;:&#34;262e6b8d86067f0f96a98611e89a05a5e28632790cadca4853976260860a02342e93a7bc8b0a0ddf2577654ff03dac5c2cc64a4c7497eafa35edfefd161e946d&#34;,
   &#34;resource-type&#34;:&#34;docker-image&#34;,
   &#34;resource-version&#34;:&#34;{\&#34;digest\&#34;:\&#34;sha256:869f748b9399b1650abe41108c11d2817bc5d4c2b226799b3041ca74bf3f88ca\&#34;}&#34;
}

$ cat ttl.json
{&#34;ttl&#34;:0,&#34;expires_at&#34;:1477060154}
}

$ date -d @1477060154
Fri Oct 21 14:29:14 UTC 2016
</code></pre><p>So here was a container that definitely should have died off a long time ago - it was Thursday 27th when we were doing this investigation. But what about the other volume?</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ stat /var/vcap/data/baggageclaim/volumes/live/1a657656-9647-4bee-54be-8505eb323b9e
stat: cannot stat ‘/var/vcap/data/baggageclaim/volumes/live/1a657656-9647-4bee-54be-8505eb323b9e’: No such file or directory
</code></pre><p>Checking the Baggage Claim logs in <code>/var/vcap/sys/log/baggageclaim</code> showed that the volume had been intentionally deleted:</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ grep -e 1a657656-9647-4bee-54be-8505eb323b9e baggageclaim.std*
baggageclaim.stdout.log:{
   &#34;timestamp&#34;:&#34;1477596534.502955675&#34;,
   &#34;source&#34;:&#34;baggageclaim&#34;,
   &#34;message&#34;:&#34;baggageclaim.tick.reaping&#34;,
   &#34;log_level&#34;:1,
   &#34;data&#34;:{
      &#34;handle&#34;:&#34;1a657656-9647-4bee-54be-8505eb323b9e&#34;,
      &#34;session&#34;:&#34;69393&#34;,
      &#34;ttl&#34;:300
   }
}
</code></pre><p>There was however no mention of the container in the ATC logs:</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session>$ zgrep eb8f2bbf-86e1-4603-4881-548c9133aa62 * | wc -l
0
</code></pre><p>&mldr;and so ends the trail. We&rsquo;ve not been able to figure out exactly what was going on. We had originally theorised that this was a one-off event caused by a Worker restart a week previously, whereby the Worker didn&rsquo;t connect to the TSA on the ATC for a good hour (again, we don&rsquo;t know why, but the debug session for that is a whole other blog post complete with voodoo exorcism). This theory didn&rsquo;t hold up to scrutiny however, as when we looked at the start times for processes relating to our containers we saw that &lsquo;orphans&rsquo; had been being created up until the present:</p><pre tabindex=0><code class=language-shell_session data-lang=shell_session># Ran at about midday
$ ps -ef --sort=start_time | grep &#34;/proc/self/exe&#34;
# SNIP!
root     30704     1  0 Oct27 ?        00:00:00 /proc/self/exe init
root      5486     1  0 Oct27 ?        00:00:00 /proc/self/exe init
root     21706     1  0 Oct27 ?        00:00:00 /proc/self/exe init
root     32505     1  0 Oct27 ?        00:00:00 /proc/self/exe init
root      1629     1  0 04:10 ?        00:00:00 /proc/self/exe init
root     20108     1  0 09:48 ?        00:00:00 /proc/self/exe init
root     20853     1  0 09:48 ?        00:00:00 /proc/self/exe init
root     22398     1  0 09:48 ?        00:00:00 /proc/self/exe init
root     31571     1  0 09:51 ?        00:00:00 /proc/self/exe init
root      1366     1  0 09:51 ?        00:00:00 /proc/self/exe init
root      8974     1  0 09:53 ?        00:00:00 /proc/self/exe init
root     10608     1  0 09:54 ?        00:00:00 /proc/self/exe init
root     13974     1  0 09:54 ?        00:00:00 /proc/self/exe init
root     24614     1  0 09:57 ?        00:00:00 /proc/self/exe init
root     27895     1  0 09:58 ?        00:00:00 /proc/self/exe init
root     30103     1  0 09:59 ?        00:00:00 /proc/self/exe init
root     30769     1  0 09:59 ?        00:00:00 /proc/self/exe init
</code></pre><p>It&rsquo;s a little odd how there are nearly no orphans overnight but as soon as office hours start, we see them being created every few minutes.</p></div><div class="blog-page_banner clearfix"><div class=blog_banner-text><h2>Get in touch</h2><p>See how much we can help you.<br>Call <span>+44 (0) 20 7846 0140</span> or</br></p><a href=mailto:contact@engineerbetter.com class=cloud-btn2><p>Contact us <span>&#xe900;</span></p></a></div></div></section><footer><div class=footer-background></div><div class="footer_wrap clearfix"><div class=footer_column-list-desktop><ul class=list><li><a href=/how-we-work/><span>&mdash;</span> How we work</a></li><li><a href=/our-services/><span>&mdash;</span> Our services</a></li><li><a href=/success-stories/><span>&mdash;</span> Success stories</a></li><li><a href=/why-cloud-native/><span>&mdash;</span> Why Cloud-Native?</a></li><li><a href=/about-us/><span>&mdash;</span> About us</a></li></ul></div><div class=footer_column-list><ul class=list><li><a href=mailto:contact@engineerbetter.com><span>&mdash;</span> Contact us</a></li><li><a href=/blog/><span>&mdash;</span> Blog</a></li><li><a href=/join-our-team/><span>&mdash;</span> Join our team</a></li><li><a href=//www.iubenda.com/privacy-policy/8251825 class="iubenda-nostyle no-brand iubenda-embed" title="Privacy Policy"><span>&mdash;</span> Privacy Policy</a><script type=text/javascript>(function(e,t){var n=function(){var e=t.createElement("script"),n=t.getElementsByTagName("script")[0];e.src="//cdn.iubenda.com/iubenda.js",n.parentNode.insertBefore(e,n)};e.addEventListener?e.addEventListener("load",n,!1):e.attachEvent?e.attachEvent("onload",n):e.onload=n})(window,document)</script></li></ul></div><div class=footer_column-icon-wrap><a href=https://twitter.com/EngineerBetter class=footer_column-icon><img class=footer_column-icon-img alt=Brand src=/img/icon-twitter.png></a>
<a href=https://github.com/EngineerBetter class=footer_column-icon><img class=footer_column-icon-img alt=Brand src=/img/icon-git-hub.png></a></div><div class=footer_column-text><p>&copy; Copyright EngineerBetter Ltd</p><p>Registered England & Wales 10141478</p></div></div></footer><script src="/js/jquery-1.11.3.min.js?v=4"></script>
<script src=/js/site.js></script></body></html>